{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SC22 Tutorial: Efficient Distributed GPU Programming for Exascale\n",
        "\n",
        "-   Time: Monday, 14 November 2022 8:30AM - 5PM CST\n",
        "-   Location: D163, Kay Bailey Hutchison Convention Center Dallas\n",
        "-   Program Link:\n",
        "    https://sc22.supercomputing.org/presentation/?id=tut102&sess=sess196\n",
        "\n",
        "## Hands-On 3: Multi-GPU Parallelization with CUDA-aware MPI\n",
        "\n",
        "### Task: Parallelize Jacobi Solver for Multiple GPUs using CUDA-aware MPI\n",
        "\n",
        "#### Description\n",
        "\n",
        "The purpose of this task is to use CUDA-aware MPI to parallelize a\n",
        "Jacobi solver. The starting point of this task is a skeleton\n",
        "`jacobi.cu`, in which the CUDA kernel is already defined and also some\n",
        "basic setup functions are present. There is also a single-GPU version\n",
        "with which the performance and numerical results are compared. Take some\n",
        "time to get familiar with the code. Some functions (like NVTX) will be\n",
        "explained in next sessions. They can be ignored for now (e.g. the `PUSH`\n",
        "and `POP` macros). Once you are familiar with the code, please work on\n",
        "the `TODOs` in `jacobi.cu`:\n",
        "\n",
        "-   Get the available GPU devices and use it and the local rank to set\n",
        "    the active GPU for each process\n",
        "-   Compute the top and bottom neigbhors. We are using\n",
        "    reflecting/periodic boundaries on top and bottom, so rank0’s Top\n",
        "    neighbor is (size-1) and rank(size-1) bottom neighbor is rank 0\n",
        "-   Use MPI_Sendrecv to exchange data between the neighbors\n",
        "    -   use CUDA-aware MPI, so the send - and the receive buffers are\n",
        "        located in GPU-memory\n",
        "    -   The first newly calculated row (‘iy_start’) is sent to the top\n",
        "        neigbor and the bottom boundary row (`iy_end`) is received from\n",
        "        the bottom process.\n",
        "    -   The last calculated row (`iy_end-1`) is send to the bottom\n",
        "        process and the top boundary (`0`) is received from the top\n",
        "    -   Don’t forget to synchronize the computation on the GPU before\n",
        "        starting the data transfer\n",
        "    -   use the self-defined MPI_REAL_TYPE. This allows an easy switch\n",
        "        between single- and double precision\n",
        "\n",
        "Compile with\n",
        "\n",
        "``` bash\n",
        "make\n",
        "```\n",
        "\n",
        "Submit your compiled application to the batch system with\n",
        "\n",
        "``` bash\n",
        "make run\n",
        "```\n",
        "\n",
        "## Advanced Task: Optimize Load Balancing\n",
        "\n",
        "### Description\n",
        "\n",
        "-   The work distribution of the first task is not ideal, because it can\n",
        "    lead to the process with the last rank having to calculate\n",
        "    significantly more than all the others. Therefore, the load\n",
        "    distribution is to be optimized in this task.\n",
        "-   Compute the `chunk_size` that each rank gets either (ny - 2) / size\n",
        "    or (ny - 2) / size + 1 rows.\n",
        "-   Compute how many processes get (ny - 2) / size resp (ny - 2) /\n",
        "    size + 1 rows\n",
        "-   Adapt the computation of (`iy_start_global`)"
      ],
      "id": "3d05d696-bb33-451b-82b6-267d89b2bbec"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  }
}
